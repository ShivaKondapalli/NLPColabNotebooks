{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification with Recurrent Neural Networks - RNN and GRU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaKondapalli/NLPColabNotebooks/blob/master/Classification_with_Recurrent_Neural_Networks_RNN_and_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfxyQrJuf_QL",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learnng for Natural Language Processing - I\n",
        "\n",
        "\n",
        "## Introduction \n",
        "\n",
        "In this Notebook, we classify the names of people to thier nationality.\n",
        "\n",
        "The code reference for this notebook is [Names to Nationality PyTorch](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
        "\n",
        "Further,  two reucurrent models: *Vanilla RNNs'* and the* Gated Reucrrent Unit * are trained on the same data and a comparison  of their * Confusion matriices* is performed. \n",
        "\n",
        "Let us start off by delineating how RNN's differ from fully connected or convolutional models and how a gru network is a variation on the vanilla Rnn that was created to address some of it's limitations. \n",
        "\n",
        "# Recurrent Neural Networks\n",
        "\n",
        "## Fully Connected Neural Network\n",
        "\n",
        "A fully connected Neural Network passes an n-dimentional input vector through a series of *Linear transformations*\n",
        "followed by a pointwise *Non-Linearity*, could be ReLU, Tanh or Sigmoid. Their is no feeding of any output back into the \n",
        "input. \n",
        "\n",
        "Here is a picture of a fully connected Neural Network. \n",
        "\n",
        "![Image source](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS6D4knBcTXoWbJgrngE_VcFqK6gsfnrssYnAs7Ts-Ee9nNDCSJ)\n",
        "\n",
        "The above pircture is from the paper: [Tropical Geometry of Deep Neural Networks](https://arxiv.org/abs/1805.07091)\n",
        "\n",
        "The above paper is highly recommended. Reading it currently and hope to write about it soon. \n",
        "\n",
        "A recurrent netwokr operates on a sequences of data as opposed to a single instance. \n",
        "\n",
        "Ex: Happy. \n",
        "\n",
        "The text example given above is a word, which can be interpreted as a sequence of characters. The RNN operates on each character of this string. Each input is called a \"*timestep*\". \n",
        "\n",
        "Now, the structure of a languge is such that it is compositional. \n",
        "Every successive character in a word, or every successive word in a sentence is dependent on the previous character or word. \n",
        "\n",
        "Recurrent neural neworks operate on a  sequence of such inputs and pass each previous input to the next time step.\n",
        "\n",
        "**Time step 1**:  \"H\" as the input to our network - timestep 1\n",
        "\n",
        "**Time step 2** : \"a\" along with the previous input H is what the second time step recieves. This is called the *hidden state*  of the network.\n",
        "\n",
        "**Time step 2**: This one gets \"a\" from the previous state and \"p\" the current input\n",
        "\n",
        "\n",
        "A picture will clear things up. \n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png)\n",
        "\n",
        "Image Source: [Chirs Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "This is a *rolled* version of an RNN, at each time step t, it gets an input $\\ {x_t}$ and  the output of the previous hidden state $\\ {h_{t-1}}$.  \n",
        "\n",
        "A linear mapping of the current input and previous hidden state is passed into a hyperbloc tangent function to predict the current hidden state  $\\ {h_t}$.\n",
        "\n",
        "$\\ {h_t = \\tanh(W_x* x_t + b_x + W_h* h_{t-1} + b_h)}$\n",
        "\n",
        "This hidden state is propogated to the next time step. This gets combined with the current input at that time step to predict the current hidden state for that time step.\n",
        "\n",
        "An *unrolled* version can make things clearer. \n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "Image Source: [Chirs Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "1. A the first time step, $\\ {h_0}$ & $\\ {x_0}$ are initialized to zero. \n",
        "\n",
        "2. The output from this unit is propogated to he the next time step. \n",
        "\n",
        "3. This time step gets the current input, i.e. the first character \"H\" and the previous hidden state.\n",
        "\n",
        "4. The next time step takes the current chracter \"a\"  and the previous hidden state for the letter '\"H\"'.\n",
        "\n",
        "5. This is propogated till the very last character of the sequence is exhausted. \n",
        "\n",
        "\n",
        "Since we are preforming classification. We take the last Hidden output and pass it through a linear layer and then a softmax to get a probability distribution over the characters. \n",
        "\n",
        "## Limitations of Vanilla RNN's.\n",
        "\n",
        "Language is a tricky thing. Let us take the following example into consideration. \n",
        "\n",
        "'\"He was a really thin and pale, had no strength at all, and for this reason the doctor conducted a number \n",
        "of tests on him. \"\n",
        "\n",
        "The last word \"him\" in the sentence is dependent on the very first word He, these are called \"Long range dependcies\", where a word in sentence exists because of some word at the beginning of the sentence. \n",
        "\n",
        "A vanilla rnn has information only from the previous time step, if it were operating on long sequence of data, it is very likely to forget what it saw earlier. We thus need some regulatory mechanism to remeber information in our sentences so that they  can be passed on to the next time step or forgotten if they are not important. \n",
        "\n",
        "Tow variations on RNN's have been devised for this. In this notebook we talk of the first. \n",
        "\n",
        "## Gated Recurrent Unit.\n",
        "\n",
        "The Gated Recurrent Unit is one such variation of a recurent neural network.\n",
        "\n",
        "In this network, we have contsructs called \"*gates*\" which control the flow of information. A GRU has three gates \n",
        "\n",
        "1. Reset : $\\ {r_t = \\sigma(W_{ir} * x_t + b_{ir} + W_{hr} * h_{t-1} + b_{hr})}$\n",
        "2. Update : $\\ {u_t  = \\sigma(W_{iu} * x_t + b_{iu} + W_{hu} * h_{t-1} + b_{hu})}$\n",
        "3. New gate : $\\ {n_t = \\tanh(W_{in} * x_t + b_{in} + r_t * W_{hn} * h_{t-1} + b_{hn})}$\n",
        "\n",
        "Hidden State: $\\ {h_t = (1- u_t) * n_t + u_t * h_{t-1}}$\n",
        "\n",
        "What is this equations tellin us? Well the Update gate and reset gate both have sigmoid functions. \n",
        "\n",
        "This means thier values lie between 0 and 1.\n",
        "\n",
        "The reset gate in equation 2 mesaures the importance of the previous hidden step in our new gate. \n",
        "\n",
        "If the update gate is close to 1, the new gate will get closer to zero. This is telling the network to retain the information from the previous hidden state.\n",
        "\n",
        "This is: $\\ {h_t = u_t * h_{t-1}}$ sicne $\\ {u_t \\approx 0}$ the hidden state value is retained, \n",
        "\n",
        "If on the other hand, the new gate was close to zero, the information woule be overwridden with the new hidden state.\n",
        "\n",
        "Now, of course the gate values are not strictly binary, but thinking about them as such gives us a conceptual handle on how information gets apassed in our network.\n",
        "\n",
        "This is precisely what allows for regulating information flow in a GRU. This inturn allows for long range dependecies. \n",
        "A more powerful model called the LSTM model, has more gates and is even better for modelling long range dependencies. \n",
        "\n",
        "We shall talk about them in a later post. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkIyGnp_5KP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unicodedata\n",
        "import string\n",
        "from io import open\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import os\n",
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TO4v9ru3MWL",
        "colab_type": "code",
        "outputId": "ec18408d-b999-488b-a3d5-3249498302ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check for GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('The power of the GPU is with oyou')\n",
        "else:\n",
        "  print('Do  not bother running this')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The power of the GPU is with oyou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kychIXMiGQbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get path and extension\n",
        "\n",
        "folder_path = \"sample_data\"\n",
        "ext = \"*.txt\"\n",
        "\n",
        "'sample_data/Arabic.txt'\n",
        "\n",
        "def get_files(path):\n",
        "    return glob.glob(os.path.join(folder_path, ext))\n",
        "\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "def unicodetoascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters)\n",
        "\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_names = {}\n",
        "all_categories = []\n",
        "\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readfiles(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodetoascii(line) for line in lines]\n",
        "\n",
        "# loop through the files.                      \n",
        "for f in get_files(folder_path):\n",
        "  category = f.split('/')[1].split('.')[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readfiles(f)\n",
        "  category_names[category] = lines\n",
        "  \n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozdWwj_fFZOk",
        "colab_type": "text"
      },
      "source": [
        "## Text Pre-Processing \n",
        "\n",
        "We need to conert our tensors to one-hot vectors.\n",
        "This is because neural networks can't take characters directly as input. They need numbers. \n",
        "\n",
        "One hot vectors aren't the best representation for text, they are sparse and don't really capture any semantic similarity \n",
        "between words.\n",
        "\n",
        "**Word Embeddings** are a much better representation for text as they are dense, i.e. each dimension of the vector captures a certain quantiative measure of the word we are interested in. \n",
        "\n",
        "They are a little out of scope for this post at the moment, but will surely be convered later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e1ZaFun3Hno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting names to tensors\n",
        "# Here we do some pre-processing to convert names to tenosrs\n",
        "\n",
        "def lettertoindex(l):\n",
        "    \"\"\"converts letter to index\"\"\"\n",
        "    return all_letters.index(l)\n",
        "\n",
        "\n",
        "def lettertotensor(l):\n",
        "    \"\"\"converts a letter to a tensor\"\"\"\n",
        "    tensor = torch.zeros(1, len(all_letters))\n",
        "    l_idx = lettertoindex(l)\n",
        "    tensor[0][l_idx] = 1\n",
        "    return tensor\n",
        "\n",
        "def nametotensor(name):\n",
        "    \"\"\"converts a name into a tensor of shape seq, 1, len(all_letters)\"\"\"\n",
        "    tensor = torch.zeros(len(name), 1, len(all_letters))\n",
        "    for idx, l in enumerate(name):\n",
        "        tensor[idx][0][lettertoindex(l)] =1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def categoryfromoutput(output):\n",
        "    top_v, top_i = output.topk(1)\n",
        "    cat_i = top_i[0].item()\n",
        "    return all_categories[cat_i], cat_i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDNjQgrVIQnB",
        "colab_type": "text"
      },
      "source": [
        "## Two Models: RNN & GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efgAdRj-8jyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is the class of our RNN network\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        combined = torch.cat((x, hidden), dim=1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def inithidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers =num_layers\n",
        "\n",
        "        self.gru  = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(1)\n",
        "\n",
        "        hidden = self.inithidden(batch_size)\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        fc_out = self.fc(hidden)\n",
        "        return fc_out\n",
        "\n",
        "    def inithidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "# instantiating our data \n",
        "\n",
        "n_hidden = 128\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\n",
        "gru = GRU(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDcUXXxXHS3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defined random choice to get a random number of examples for our data\n",
        "def random_choice(lst):\n",
        "    return lst[np.random.randint(0, len(lst)-1)]\n",
        "\n",
        "\n",
        "def randomtrainningexample():\n",
        "    category = random_choice(all_categories)\n",
        "    name = random_choice(category_names[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    name_tensor = nametotensor(name)\n",
        "    return category, name, category_tensor, name_tensor\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiabsx3FIH0-",
        "colab_type": "text"
      },
      "source": [
        "## Training \n",
        "\n",
        "We will train a plain vanilla RNN and  A GRU network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6PZ6fCTHsxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting hyperparameters for our data \n",
        "\n",
        "# setting hyperparameters\n",
        "learning_rate = 0.005\n",
        "criterion_rnn = nn.NLLLoss()\n",
        "criterion_gru = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_rnn(category_tensor, name_tensor):\n",
        "    hidden = rnn.inithidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    for i in range(name_tensor.size()[0]):\n",
        "        output, hidden = rnn.forward(name_tensor[i], hidden)\n",
        "\n",
        "    loss = criterion_rnn(output, category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)  # can also use torch.optim() if you so choose to\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "\n",
        "def train_gru(category_tensor, name_tensor):\n",
        "\n",
        "    gru.zero_grad()\n",
        "\n",
        "    output = gru.forward(name_tensor)\n",
        "\n",
        "    loss = criterion_gru(output.squeeze(1), category_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    for p in gru.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)  # can also use torch.optim() if you so choose to\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "\n",
        "def time_taken(start):\n",
        "    time_elapsed = time.time() - start\n",
        "    min = time_elapsed//60\n",
        "    sec = time_elapsed%60\n",
        "    return '%dm %ds' % (min, sec)\n",
        "\n",
        "\n",
        "def evaluate(name_tensor, model):\n",
        "\n",
        "    if model == rnn:\n",
        "\n",
        "        hidden = rnn.inithidden()\n",
        "\n",
        "        for i in range(name_tensor.size()[0]):\n",
        "            output, next_hidden = rnn.forward(name_tensor[i], hidden)\n",
        "\n",
        "    elif model == gru:\n",
        "\n",
        "        output = gru.forward(name_tensor)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def predict(name, model, n_predictions=3):\n",
        "    print(name)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(nametotensor(name), model)\n",
        "\n",
        "        if model == gru:\n",
        "            output = output.squeeze(1)\n",
        "\n",
        "        top_n, top_i = output.topk(n_predictions, 1, True)\n",
        "        predictions_lst = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            val = torch.exp(top_n[0][i]) if model == rnn else top_n[0][i]\n",
        "            cat_idx = top_i[0][i].item()\n",
        "            print(f'Value: {val.item()}, language: {all_categories[cat_idx]}')\n",
        "            predictions_lst.append([val, all_categories[cat_idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82FzHwNIhrJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# # Confusion Matrix :\n",
        "\n",
        "We plot confusion matrices for both the network we train. \n",
        "\n",
        "The principal diagonal of the matrix shows us the correct predictions. The brighter the digonal the better the model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeEycEQDIYhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "    \n",
        "current_loss_rnn = 0\n",
        "all_losses_rnn = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "print('Training Vanilla RNN')\n",
        "print(' ')\n",
        "\n",
        "# Training Vanilla RNN\n",
        "for i in range(1, n_iters+1):\n",
        "  \n",
        "  category, name, category_tensor, name_tensor = randomtrainningexample()\n",
        "  output, loss = train_rnn(category_tensor, name_tensor)\n",
        "  \n",
        "  current_loss_rnn += loss\n",
        "  \n",
        "  if n_iters % print_every == 0:\n",
        "    pred, pred_i = categoryfromoutput(output)\n",
        "    prediction = 'True' if pred == category else f'False, correct one is {category}'\n",
        "    \n",
        "    print('%d %d%% (%s) %.4f %s / %s %s' % (i, i / n_iters * 100, time_taken(start), loss, name, pred, prediction))\n",
        "    \n",
        "  if i % plot_every == 0:\n",
        "    all_losses_rnn.append(current_loss_rnn/plot_every)\n",
        "    current_loss_rnn = 0\n",
        "\n",
        "print('')\n",
        "print('############################################################################')\n",
        "print('Training GRU network now')\n",
        "\n",
        "# Training Gated Recurrent Unit\n",
        "current_loss_gru = 0\n",
        "all_losses_gru = []\n",
        "\n",
        "for i in range(1, n_iters+1):\n",
        "  category, name, category_tensor, name_tensor = randomtrainningexample()\n",
        "  output, loss = train_gru(category_tensor, name_tensor)\n",
        "  current_loss_gru += loss\n",
        "  \n",
        "  if n_iters % print_every == 0:\n",
        "    pred, pred_i = categoryfromoutput(output)\n",
        "    prediction = 'True' if pred == category else f'False, correct one is {category}'\n",
        "    print('%d %d%% (%s) %.4f %s / %s %s' % (i, i / n_iters * 100, time_taken(start), loss, name, pred, prediction))\n",
        "    \n",
        "  if i % plot_every == 0:\n",
        "    all_losses_gru.append(current_loss_gru/plot_every)\n",
        "    current_loss_gru = 0\n",
        "\n",
        "# plot confusion matrix and losses\n",
        "confusion_rnn = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "# Add one to each row: the real category. Each column: the predicted category.\n",
        "# The darker the principal diagonal, the better the model.\n",
        "\n",
        "for i in range(n_confusion):\n",
        "  category, name, category_tensor, name_tensor = randomtrainningexample()\n",
        "  output_rnn = evaluate(name_tensor, rnn)\n",
        "  guess, guess_i_rnn = categoryfromoutput(output_rnn)\n",
        "  real_category_i = all_categories.index(category)\n",
        "  confusion_rnn[real_category_i][guess_i_rnn] += 1\n",
        "\n",
        "for i in range(n_categories):\n",
        "  confusion_rnn[i] = confusion_rnn[i]/confusion_rnn[i].sum()\n",
        "\n",
        "# Set up fig, axes.\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(221)\n",
        "cax = ax1.matshow(confusion_rnn.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set the labels for x and y axes\n",
        "ax1.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax1.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Major tick locations on the axis are set.\n",
        "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax1.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# Plot Vanilla Rnn losses\n",
        "ax1 = fig.add_subplot(222)\n",
        "ax1.set_title('Vanilla Rnn Losses')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Losses')\n",
        "ax1.plot(all_losses_rnn)\n",
        "\n",
        "# Gated Recurrent unit\n",
        "confusion_gru = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "# Add one to each row: the real category and each column: the predicted category.\n",
        "# The darker the principal diagonal, the better the model.\n",
        "\n",
        "for i in range(n_confusion):\n",
        "  category, name, category_tensor, name_tensor = randomtrainningexample()\n",
        "  output_gru = evaluate(name_tensor, gru)\n",
        "  guess, guess_i_gru = categoryfromoutput(output_gru)\n",
        "  real_category_i = all_categories.index(category)\n",
        "  confusion_gru[real_category_i][guess_i_gru] += 1\n",
        "\n",
        "for i in range(n_categories):\n",
        "  confusion_gru[i] = confusion_gru[i]/confusion_gru[i].sum()\n",
        "\n",
        "ax1 = fig.add_subplot(223)\n",
        "cax1 = ax1.matshow(confusion_gru.numpy())\n",
        "fig.colorbar(cax1)\n",
        "\n",
        "# Set the labels for x and y axes\n",
        "ax1.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax1.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Major tick locations on the axis are set.\n",
        "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax1.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# Plot GRU losses\n",
        "ax1 = fig.add_subplot(224)\n",
        "ax1.set_title('GRU losses')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Losses')\n",
        "ax1.plot(all_losses_gru)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AofqenwQJm21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict for vanilla Rnn\n",
        "predict('Akutagawa', rnn)\n",
        "predict('Avgerinos', rnn)\n",
        "predict('Lestrange', rnn)\n",
        "\n",
        "# predict for GRU\n",
        "predict('Akutagawa', gru)\n",
        "predict('Avgerinos', gru)\n",
        "predict('Lestrange', gru)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPA4IHpXTgox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}