{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classyfying names to Gender with LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaKondapalli/NLPColabNotebooks/blob/master/Classyfying_names_to_Gender_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F78GcwcjpQdN",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Natural Languge Processing - III\n",
        "\n",
        "# # Intorduction \n",
        "\n",
        "In this notebook, we classify names on people to their genders. The theory behind LSTM networks and how they work is described in the following two notebooks. \n",
        "\n",
        "[Vanilla Rnn and GRU](https://colab.research.google.com/drive/1SyElHeyoRAY9MtalzeyBMSCvu28bk53x)\n",
        "\n",
        "[Long Short Term memory networks](https://colab.research.google.com/drive/1w2tK7_SCHdeiV6NEPohjkhru_O3j1W1m)\n",
        "\n",
        "The interesting part of this notebook is that the data was Scraped from the web. It was then saved in a file. The code to scrape data is hosted at my github. \n",
        "\n",
        "[Web Scraper](https://github.com/ShivaKondapalli/NLPPyTorch/blob/master/Web_Scraper.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkDeSe0prbki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unicodedata\n",
        "import string\n",
        "from io import open\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import os\n",
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57X6dYhnGGj",
        "colab_type": "code",
        "outputId": "dcf598ed-f85c-43ac-e39b-927478e14f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = 'sample_data/'\n",
        "ext = '*.txt'\n",
        "\n",
        "\n",
        "def get_files(path):\n",
        "    return glob.glob(os.path.join(path, ext))\n",
        "\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "def unicodetoascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters)\n",
        "\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "sex_to_name = {}\n",
        "all_categories = []\n",
        "\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readfiles(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodetoascii(line) for line in lines]\n",
        "\n",
        "\n",
        "for f in get_files(path):\n",
        "    category = f.split('/')[1].split('.')[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readfiles(f)\n",
        "    sex_to_name[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "n_categories"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy104W5FpONF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lettertoindex(l):\n",
        "    \"\"\"converts letter to index\"\"\"\n",
        "    return all_letters.index(l)\n",
        "\n",
        "\n",
        "def lettertotensor(l):\n",
        "    \"\"\"converts a letter to a tensor\"\"\"\n",
        "    tensor = torch.zeros(1, len(all_letters))\n",
        "    l_idx = lettertoindex(l)\n",
        "    tensor[0][l_idx] = 1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def nametotensor(name):\n",
        "    \"\"\"converts a name into a tensor of shape seq, 1, len(all_letters)\"\"\"\n",
        "    tensor = torch.zeros(len(name), 1, len(all_letters))\n",
        "    for idx, l in enumerate(name):\n",
        "        tensor[idx][0][lettertoindex(l)] =1\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def categoryfromoutput(output):\n",
        "    top_v, top_i = output.topk(1)\n",
        "    cat_i = top_i[0].item()\n",
        "    return all_categories[cat_i], cat_i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqPtgVlLsr_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d95b14d-7b7c-4095-fed2-70acb662cfa3"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(1)\n",
        "\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        cell = self.cell_state(batch_size)\n",
        "\n",
        "        output, hidden = self.lstm(x, (hidden, cell))\n",
        "\n",
        "        last_output = output[-1]  # batch_size * hidden_size\n",
        "\n",
        "        fc_out = self.fc(last_output)  # 1 * hidden_size\n",
        "\n",
        "        return fc_out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "    def cell_state(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "      \n",
        "n_hidden = 256\n",
        "lstm = LSTM(n_letters, n_hidden, n_categories)\n",
        "lstm"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(57, 256)\n",
              "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhmt301rsxRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genderfromoutput(output):\n",
        "    top_v, top_i = output.topk(1)\n",
        "    cat_i = top_i[0].item()\n",
        "    return all_categories[cat_i], cat_i\n",
        "\n",
        "\n",
        "def randomtrainningexample():\n",
        "    sex = np.random.choice(all_categories)\n",
        "    name = np.random.choice(sex_to_name[sex])\n",
        "    sex_tensor = torch.tensor([all_categories.index(sex)], dtype=torch.long)\n",
        "    name_tensor = nametotensor(name)\n",
        "    return sex, name, sex_tensor, name_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlgNYdGvs_ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genderfromoutput(output):\n",
        "    top_v, top_i = output.topk(1)\n",
        "    cat_i = top_i[0].item()\n",
        "    return all_categories[cat_i], cat_i\n",
        "\n",
        "\n",
        "def randomtrainningexample():\n",
        "    sex = np.random.choice(all_categories)\n",
        "    name = np.random.choice(sex_to_name[sex])\n",
        "    sex_tensor = torch.tensor([all_categories.index(sex)], dtype=torch.long)\n",
        "    name_tensor = nametotensor(name)\n",
        "    return sex, name, sex_tensor, name_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTKnPUdVtE9_",
        "colab_type": "text"
      },
      "source": [
        "## Training \n",
        "\n",
        "We train the network and plot losses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjmJJkFVtDmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.007\n",
        "criterion_lstm = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train_lstm(sex_tensor, name_tensor):\n",
        "    lstm.zero_grad()\n",
        "\n",
        "    output = lstm.forward(name_tensor)\n",
        "\n",
        "    loss = criterion_lstm(output.squeeze(1), sex_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    for p in lstm.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)  # can also use torch.optim() if you so choose to\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "\n",
        "def evaluate(name_tensor, model):\n",
        "\n",
        "    out = model.forward(name_tensor)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def predict(name, model, n_predictions=3):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(nametotensor(name), model)\n",
        "\n",
        "        output = output.squeeze(1)\n",
        "\n",
        "        top_n, top_i = output.topk(n_predictions, 1, True)\n",
        "        predictions_lst = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            val = top_n[0][i]\n",
        "            cat_idx = top_i[0][i].item()\n",
        "            print(f'Value: {val.item()}, language: {all_categories[cat_idx]}')\n",
        "            predictions_lst.append([val, all_categories[cat_idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4s2brp4uAcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_taken(start):\n",
        "    time_elapsed = time.time() - start\n",
        "    min = time_elapsed//60\n",
        "    sec = time_elapsed%60\n",
        "    return '%dm %ds' % (min, sec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh7IEXmztNlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "    n_iters = 100000\n",
        "    print_every = 5000\n",
        "    plot_every = 1000\n",
        "\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for i in range(1, n_iters+1):\n",
        "        sex, name, sex_tensor, name_tensor = randomtrainningexample()\n",
        "        output, loss = train_lstm(sex_tensor, name_tensor)\n",
        "        current_loss += loss\n",
        "\n",
        "        if n_iters % print_every == 0:\n",
        "            pred, pred_i = genderfromoutput(output)\n",
        "            prediction = 'True' if pred == sex else f'False, correct one is {sex}'\n",
        "            print('%d %d%% (%s) %.4f %s / %s %s' % (i, i / n_iters * 100, time_taken(start), loss, name, pred, prediction))\n",
        "\n",
        "        if i % plot_every == 0:\n",
        "            all_losses.append(current_loss/plot_every)\n",
        "            current_loss = 0\n",
        "\n",
        "    confusion = torch.zeros(n_categories, n_categories)\n",
        "    n_confusion = 10000\n",
        "\n",
        "    # Add one to each row: the real category and each column: the predicted category.\n",
        "    # The darker the principal diagonal, the better the model.\n",
        "    for i in range(n_confusion):\n",
        "        sex, name, sex_tensor, name_tensor = randomtrainningexample()\n",
        "        output = evaluate(name_tensor, lstm)\n",
        "        guess, guess_i = genderfromoutput(output)\n",
        "        real_category_i = all_categories.index(category)\n",
        "        confusion[real_category_i][guess_i] += 1\n",
        "\n",
        "    for i in range(n_categories):\n",
        "        confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "    # Set up fig, axes.\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.set_title('Confusion Matrix for two classes')\n",
        "    cax = ax1.matshow(confusion.numpy())\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set the labels for x and y axes\n",
        "    ax1.set_xticklabels([''] + all_categories, rotation=90)\n",
        "    ax1.set_yticklabels([''] + all_categories)\n",
        "\n",
        "    # Major tick locations on the axis are set.\n",
        "    ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax1.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    # Plot Vanilla Rnn losses\n",
        "    ax1 = fig.add_subplot(122)\n",
        "    ax1.set_title('LSTM Losses')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Losses')\n",
        "    ax1.plot(all_losses)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6dGh-wJtW5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}